\documentclass[10pt,a4paper,oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}

\newtheorem{theoreme}{Théorème}
\newtheorem{proposition}{Proposition}
\newtheorem{corollaire}{Corollaire}
\newtheorem{lemme}{Lemme}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newenvironment{exemple}[1][Exemple]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\begin{document}

\title{Résumé MAP311}
\author{Mario Ynocente Castro}

\maketitle

\section{Introduction aux Probabilités}

\begin{itemize}
\item
\textbf{Expérience aléatoire ($\mathcal{E}$)}: une expérience qui reproduite dans des conditions identiques, peut conduire à plusieurs résultats possibles, et dont on ne peut prévoir le résultat par avance.

\item
\textbf{Espace d'états ($\Omega$)}: espace de tous les résultats possibles. Un résultat possible de l'expérience est noté $\omega (\omega \in \Omega)$.

\item
\textbf{Événement aléatoire}: sous-ensemble de $\Omega$ dont on peut dire au vu de l'expérience s'il est réalisé ou non.

\item
Nous notons par $\mathcal{A}$ l'ensemble de tous les événements. Nous pourrons prendre $\mathcal{A} = \mathcal{P}(\Omega)$, ensemble de toutes les parties de $\Omega$, mais pas toujours.

\item
Une \textbf{probabilité} sur l'espace $(\Omega, \mathcal{A})$ est une application de $\mathcal{A}$ dans $[0,1]$, notée $\mathbb{P}$, telle que:

\begin{itemize}
\item
On a $\mathbb{P}(\Omega) = 1$.
\item
Pour toute suite (dénombrable) $(A_n)_n$ d'éléments de $\mathcal{A}$ deux-à-deux disjoints, on a $\mathbb{P}(\bigcup_{n}A_{n}) = \sum_n \mathbb{P}(A_n)$
\end{itemize}

\item
On appelle le triplet $(\Omega, \mathcal{A}, \mathbb{P})$ un \textbf{espace de probabilité}.

\item
Soit $(B_i)_{i \in N}$ une \textbf{partition} finie ou dénombrable de $\Omega$ ($B_i \bigcap B_j = \emptyset$ et $\bigcup_i B_i = \Omega$).

\begin{itemize}
\item
\textbf{Formule des probabilités totales}: $\mathbb{P}(A) = \sum_i \mathbb{P}(A \bigcap B_i) = \sum_i \mathbb{P}(A|B_i) \mathbb{P}(B_i)$

\item
\textbf{Formule de Bayes}: Si $\mathbb{P}(A) > 0$,

\[ \forall i \in \mathbb{N}, \mathbb{P}(B_i|A) = \frac{\mathbb{P}(A \bigcap B_i)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B_i)\mathbb{P}(B_i)}{\sum_j \mathbb{P}(A|B_j)\mathbb{P}(B_j)} \]
\end{itemize}

\end{itemize}

\section{Variables aléatoires discrètes}

\begin{itemize}
\item
\textbf{Fonction génératrice:} Pour $X$ variable aléatoire à valeurs dans $\mathbb{N}$, et $\forall s \in [0,1]$,

\[ G_X(s) = \mathbb{E}(s^X) = \sum_{n} p_n s^n\text{, où }p_n = \mathbb{P}(X = n) \]

\item
\begin{proposition}

\begin{enumerate}
\item
$G_X$ continue sur $[0,1]$, $C^\infty$ sur $[0,1[$. Elle caractérise la loi de $X$.

\item
$X \in \mathbb{L}^1 \Leftrightarrow G_X$ dérivable (à gauche) en $s = 1$, et

\[ \mathbb{E}(X) = G'_X(1) \]

\item
$X(X - 1)\ldots(X - p) \in \mathbb{L}^1 \Leftrightarrow G_X$ est $p + 1$ fois dérivable en $1$, et

\[ \mathbb{E}(X(X - 1)\ldots(X - p)) = G_X^{(p + 1)}(1) \]

En particulier: $Var(X) = G_X''(1) + G_X'(1) - (G_X'(1))^2$.
\end{enumerate}

\end{proposition}

\item
\textbf{Variable aléatoire de Bernoulli:}
\[ \mathbb{P}(X = 0) = 1 - p,\text{ }\mathbb{P}(X = 1) = p \]
\[ G_X(s) = 1-p+ps \Rightarrow \mathbb{E}(X) = p,\text{ }Var(X) = p(1-p) \]

\item
\textbf{Variable aléatoire binomiale $\mathcal{B}(n,p)$:}

\[ \text{Pour } k \in \{ 0,\ldots,n \},\text{ }\mathbb{P}(X = k) = \binom{n}{k} p^k(1 - p)^{n - k} \]
\[ G_X(s) = (1 - p + ps)^n \Rightarrow \mathbb{E}(X) = np,\text{ }Var(X) = np(1-p) \]

\item
\textbf{Variable aléatoire géometrique:}

\item
\textbf{Variable aléatoire de Poisson:}

\end{itemize}

\section{Variables aléatoires réelles}

\begin{itemize}
\item
Si $\mathbb{E}(X^2) < +\infty$, alors $\mathbb{E}(|X|) < +\infty$ car $|X| \leq 1 + X^2$, et

\[ Var(X) = \mathbb{E}((X - \mathbb{E}(X))^2) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2 \]
\[ Var(aX + b) = a^2Var(X) \]

\item
\textbf{Variable uniforme sur $[a,b]$:}

\[ f(x) = \left\{ 
  \begin{array}{l l }
    \frac{1}{b - a} & \quad \text{si }a \leq x \leq b\\
    0 & \quad \text{sinon.}
  \end{array} \right.\]
\[ \mathbb{E}(X) = \frac{a + b}{2}, Var(X) = \frac{(b - a)^2}{12} \]

\item
\textbf{Variable exponentielle $\mathcal{E}(\lambda)$:} Soit $\lambda > 0$

\[ f(x) = \left\{ 
  \begin{array}{l l }
    0 & \quad \text{si }x < 0\\
    \lambda e^{-\lambda x} & \quad \text{sinon.}
  \end{array} \right.\]
\[ \mathbb{E}(X) = \frac{1}{\lambda}, Var(X) = \frac{1}{\lambda^2} \]

\item
\textbf{Variable normale de loi $\mathcal{N}(m,\sigma^2)$:}

\[ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp(-\frac{(x - m)^2}{2 \sigma^2}) \]
\[ \mathbb{E}(X) = m, Var(X) = \sigma^2 \]

\item
\textbf{Variable de loi gamma $\Gamma(\alpha, \theta)$}

Soit $\alpha > 0, \Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha - 1}e^{-x} dx$

\begin{itemize}
\item
$\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$
\item
$\Gamma(1) = 1$
\item
$\Gamma(n + 1) = n!$ pour tout entier $n \geq 0$

\[ f(x) = \left\{ 
  \begin{array}{l l }
    0 & \quad \text{si }x \leq 0\\
    \frac{1}{\Gamma(\alpha)} \theta^\alpha x^{\alpha - 1} e^{-\theta x} & \quad \text{sinon.}
  \end{array} \right.\]

\[ \mathbb{E}(X) = \frac{\alpha}{\theta}, Var(X) = \frac{\alpha}{\theta^2}, \mathbb{E}(X^\beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)} \frac{1}{\theta^\beta} (\beta > -\alpha) \]

\end{itemize}

\item
\textbf{Simulation d'une variable al\'eatoire par inversion de la fonction de r\'epartition}

D\'efinissons la fonction inverse continue \'a gauche de $F$ par:

\[ G(u) = \inf \{ x : F(x) \geq u \}, \forall u \in ]0,1[ \]

On a $G(u) \leq x \Leftrightarrow u \leq F(x)$, et $F(G(u)) = u$ si $0 < u < 1$ et si la fonction $F$ est continue au point $G(u)$.

\begin{proposition}
Soit $U$ une variable al\'eatoire de loi uniforme sur $[0,1]$, $F$ une fonction de r\'epartition et $G$ son inverse. La variable al\'eatoire $Y = G(U)$ est alors une variable al\'eatoire de fonction de r\'epartition $F$.
\end{proposition}

\item
$cov(X,Y) = \mathbb{E}((X - \mathbb{E}(X))(Y - \mathbb{E}(Y))) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)$

$cov(X,X) = Var(X)$

$cov(aX + b,a'Y + b') = aa'cov(X,Y)$

$Var(X_1 + \ldots + X_n) = Var(X_1) + \ldots + Var(X_n) + 2 \sum_{1 \leq i < j \leq n} cov(X_i,X_j)$

\end{itemize}

\section{Fonctions caractéristiques}

\begin{itemize}
\item
Soit $X = (X_1,\ldots,X_d)$ un vecteur aléatoire de $\mathbb{R}^d$. Sa fonction caractéristique est la fonction de $\mathbb{R}^d$ dans $\mathbb{C}$ définie par:

\[ u = (u_1,\ldots,u_d) \mapsto \phi_X(u) = \mathbb{E}(e^{i<u,X>}) \]
-%Si $X$ est une variable à valeurs dans $\mathbb{R}^n$, sa fonction caractéristique est la fonction $\phi_X$ de $\mathbb{R}^n$ dans $C$ définie par: $\phi_X(u) = \mathbb{E}(e^{i \langle u, X \rangle})$

\item
\begin{proposition}
La fonction $\phi_X$ est de module inférieur ou égal a 1, continue, avec: $\phi_X(0) = 1; \phi_X(-u) = \overline{\phi_X(u)} = \phi_{-X}(u)$.
\end{proposition}

\item
Si $X$ est une v.a. réelle à densité $p_X$, $\phi_X(u) = \mathbb{E}(e^{iuX}) = \int_{\mathbb{R}} e^{iux} p_X(x)dx$

\item
Si $X$ est à valeurs entières: $\phi_X(u) = G_X(e^{iu}) = \sum_{k = 0}^{+\infty} \mathbb{P}(X = k) e^{iuk}$

\item
\textbf{X suit une loi binomiale $\mathcal{B}(n,p)$:} $\phi_X(u) = (pe^{iu} + 1 - p)^n$

\item
\textbf{X suit une loi de Poisson de param\'etre $\theta$:} $\phi_X(u) = e^{\theta(e^{iu} - 1)}$

\item
\textbf{X suit une loi uniforme sur $[a,b]$:} $\phi_X(u) = \frac{e^{iub} - e^{iua}}{iu(b - a)}$

\item
\textbf{X suit une loi exponentielle $\mathcal{E}(\lambda)$:} $\phi_X(u) = \frac{\lambda}{\lambda - iu}$

\item
\textbf{X suit une loi normale $\mathcal{N}(m,\sigma^2)$:} $\phi_X(u) = e^{ium-u^2\sigma^2/2}$

\item
\textbf{X suit une loi Gamma $\Gamma(\alpha,\lambda)$:} $\phi_X(u) = (\frac{\lambda}{\lambda - iu})^\alpha$

\item
\begin{theoreme}
La fonction caract\'erisitique $\phi_X$ caract\'erise la loi du vecteur al\'eatoire $X$. Ainsi, si deaux vecteurs al\'eatoires $X$ et $Y$ ont même fonction caract\'eristique, ils ont même loi.
\end{theoreme}

\item
\begin{proposition}
Si $X$ et $Y$ sont deux vecteurs al\'eatoires ind\'ependants \`a valeurs dans $\mathbb{R}^n$, la fonction caract\'eristique de la somme $X + Y$ est donn\'ee par: $ \phi_{X + Y} = \phi_X \phi_Y$
\end{proposition}

\item
Soit $X,Y$ ind\'ependantes, et $Z = X + Y$:

\begin{itemize}
\item
$X$ suit loi normale $\mathcal{N}(m,\sigma^2)$ ey $Y$ suite une loi $\mathcal{N}(m',(\sigma')^2)$.

Alors $Z$ suit une loi $\mathcal{N}(m + m',\sigma^2 + (\sigma')^2)$.

\item
$X$ et $Y$ suivent des lois de Poisson de param\`etres $\theta$ et $\theta'$.

Alors $Z$ suit une loi de Poisson de param\`etre $\theta + \theta'$.

\item
$X$ suit loi binomiale $\mathcal{B}(n,p)$ ey $Y$ suite une loi $\mathcal{B}(m,p)$.

Alors $Z$ suit une loi $\mathcal{B}(n + m,p)$.
\end{itemize}

\item
Un vecteur al\'eatoire $X = (X_1,\ldots,X_n)$ \`a valeurs dans $\mathbb{R}^n$ est appel\'e un vecteur gaussien si toute combinaison lin\'eaire $\sum_{j = 1}^n a_j X_j = \langle a, X \rangle$, pour $a = (a_1,\ldots,a_n) \in \mathbb{R}^n$, suit une loi normale (avec la convention que la masse de Dirac au point $m$ est la loi normale $\mathcal{N}(m,0)$)

\end{itemize}

\section{Applications du Théorème de la Limite Centrale}

\begin{theoreme}
Soit $(X_n)_n$ une suite de v.a. réelles indépendantes, de même loi, de carré intégrable: $m = \mathbb{E}(X_1)$ et $\sigma^2 = \mathbb{V}ar(X_1)$. Alors

\[ \sqrt{n}(\frac{1}{n}\sum_{k = 1}^n X_k - m) \]

converge en loi (quand $n \to +\infty)$ vers une variable aléatoire de loi normale $N(0,\sigma^2).$
\end{theoreme}


\begin{corollaire}
Pour tout $a \leq b$,

\[ \lim\limits_{n \to +\infty} \mathbb{P}( a \leq \frac{\sqrt{n}}{\sigma}(\frac{1}{n}\sum_{k = 1}^n X_k - m) \leq b ) = \int_{a}^{b}  \frac{e^{ -x^2/2 }}{\sqrt{2 \pi }} dx \]
\end{corollaire}

\end{document}
